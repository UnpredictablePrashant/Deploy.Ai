# Deploy.Ai

 This is an AI Deployment Planner. Users can converse with an AI Bot that is specially trained to provide you a personalized plan for your deployment needs. This project is divided into three parts, this is the main repository and there are three other repositories which are the subset of this repository. 

1. Landing Page and Authentication
2. Deployment Plan Created By AI
3. Deployment Performed by Python Automation

## Part 1: Landing Page with Authentication, User Profiles and Chatbot features
For this purpose we are using plain html (index.html),css and javascript, along with Amazon Cognito for authentication. User will use email id and verification to start using the bot. For the Purpose of bot features/frontend we will be using Kommunicate or kore.ai. It will either be a standalone solution or be a frontend for lex or chatgpt. 


## Part 2: Deployment Plan Created by AI

For this purpose we use two AI agents, one is ChatGPT's custom bot creation feature and the other is Amazon Lex. We built these AI agents by feeding various self created custom knowledge base. The bot is designed to interact with the user and ask relevent questions about their deployment requirements. Based upon these requirements the bot generates a deployment plan. It also requests the users if they want to deploy the plan. Upon confirmation, the bot generates file.json that automatically gets stored in json_dumps folder (or possibly in an s3 bucket in the future). It also stores in (deployment_architecture) and sends an architecture drawing to the users slack account. (whimsical maybe used for diagram generation)

Things to do (part 2):
Another file that contains secrets may also get generated by the bot, the details of it will be used by tf scrpits.


## Part 3: Deployment Performed by Python

In part three we created the json_read.py file that looks for any json file from the json_dumps folder. If there is any, then it compares the contents against the contents of all the files in the folder json_library. If it finds a match then that file number becomes the result. We have set conditions the run based on the file number we get on the result. For example, the result is 101, then the programs instructs the os to change directory into the terraform_scripts/101 directory and then applies the corresponding terraform script.

Once the infra is created by terraform, the ansible scripts are used to deploy and ensure the website works.

## Where is this Main Project Deployed?

We are using an ec2 instance with an elastic IP attached to it. Github actions is used to update the local repository. Nginx is set up with a domain name. We are also using cloudflare for this purpose. For SSL we are using certbot. 

### Are you well architected?

We are using the AWS's Well Architected Framework to create this project which ensures operational excellence, performance efficency, reliability, sustainability, security and cost optimization. 



## Repository Structure:

Files:

* /json_read.py - main file that reads the contents of the json file and matches with existing deployment json's. Then runs the corresponding infrastructure scripts and other scripts necessary for deployment. 

* /json_read_starter.py - 


* /website/index.html - landing page for the website

Things To Do:

0. S3 bucket instead of dumps folder
1. Compare json against dynamoDB/documentDB instead of other jsons in the library.
2. Create more Json files and store it in spare library of more use cases.
3. Make json_read file more classy with functions and classes.
After the infra is created and the application is deployed, users get to see the evidence with either system manager or resource explorer. 

Future Plans:
Currently in the json_read.py file, we are creating infrastructure and config based on conditions written for each json deployment case.

But later, we would like to create an AI model that will be used to automatically figure out the corresponding codes to run by reading the Terraform scripts.

Or even better, we would build the AI model by training it on various TF scripts so that it would automatically create the Terraform scripts.

With the input provided by the user, this AI model will detect the type of deployment.

1. By analyzing the JSON file, this AI model would be able to identify patterns and dependencies within the infrastructure and configuration requirements. This would eliminate the need for manual coding and reduce human errors, resulting in a more efficient and accurate deployment process. Additionally, the AI model could continuously learn and adapt to new deployment scenarios, enhancing its capabilities over time. 

2. The AI model would then translate these natural language inputs into the appropriate Terraform scripts, further simplifying the deployment process for users. deployment case in the json_read.py file.

3. Discussing the future vision: Explain how there is a plan to develop an AI model that can automatically determine the corresponding codes to run by reading Terraform scripts, which would eliminate the need for manual intervention.

4. Highlighting improved efficiency: Elaborate on how building an AI model by training it on various TF scripts would lead to greater automation and efficiency in creating Terraform scripts, as it would be able to generate them automatically.

5. Describing user interaction: Write about how this proposed AI model could incorporate user input to detect different types of deployments accurately, making it more versatile and adaptable for various scenarios.

6. Addressing potential benefits: Explore the potential advantages of implementing such an AI-driven system, including reduced human error, a faster script generation process, scalability, and better resource allocation based on specific deployment requirements.




